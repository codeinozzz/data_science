{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div\n",
    "  style=\"\n",
    "    background-color: #f0f0f0;\n",
    "    color:rgb(56, 56, 56);\n",
    "    padding: 8px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    gap: 100px;\n",
    "  \"\n",
    ">\n",
    "  <img src=\"./images/brand.svg\" style=\"max-height: 80px;\">\n",
    "  <strong>\n",
    "    AI Saga: Data Science and Machine Learning</br>\n",
    "    3.lab.2. Auto MPG Regression - Neural Networks\n",
    "  </strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from rich.table import Table\n",
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "al igual que la actividad pasada aqui vamos hacer priemro las configuraciones \n",
    "\n",
    "> nota pues solo explicare los mas importante \n",
    "\n",
    "- torch.manual_seed(42) fija las semillas aleatorias para resultados reproducibles\n",
    "- np.random.seed(42) mismos pesos iniciales y división de datos en cada ejecución\n",
    "\n",
    "### codificacion de las variables\n",
    "\n",
    "- label_encoder = LabelEncoder()\n",
    "- df[\"origin_encoded\"] = label_encoder.fit_transform(df[\"origin\"])\n",
    "\n",
    "\n",
    "basicameente estas lienas convierten las variables a numero pues como y sabemos las redes neuronales solo leen numeros\n",
    "\n",
    "\n",
    "### Variables \n",
    "\n",
    "- X = df[[\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin_encoded\"]].values\n",
    "- y = df[\"mpg\"].values\n",
    "\n",
    "aqui simplemntes se seleccionan 7 caracteristicas para la entrada y en y se define la variable objetivo que para este caso sera MPG-Milla por galon\n",
    "\n",
    "\n",
    "### Normalizacion: \n",
    "\n",
    "- scaler = StandardScaler()\n",
    "- X_scaled = scaler.fit_transform(X)\n",
    "- X_train, X_test, y_train, y_test = train_test_split\n",
    "\n",
    "\n",
    "aqui son dos funciones las principales que son StandardScaler: Normaliza características (media=0, desviación=1) y train_test_split: que define 80% entrenamiento, 20% prueba\n",
    "\n",
    "\n",
    "### Tensores: \n",
    "\n",
    "\n",
    "- X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "- y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "aqui basicamente se convierte a la forma en como fuciona la libreira PyT orch qu es [n] a [n, 1]\n",
    "\n",
    "\n",
    "\n",
    "### Anaalisis\n",
    "\n",
    "en resumen aqui preparamos los datos para el entrenamiento de las redes \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Cargar dataset desde seaborn\n",
    "df = sns.load_dataset(\"mpg\")\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"origin_encoded\"] = label_encoder.fit_transform(df[\"origin\"])\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = df[\n",
    "    [\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "        \"weight\",\n",
    "        \"acceleration\",\n",
    "        \"model_year\",\n",
    "        \"origin_encoded\",\n",
    "    ]\n",
    "].values\n",
    "y = df[\"mpg\"].values\n",
    "\n",
    "## Preprocesamiento de Datos\n",
    "# Normalizamos las características para mejorar el entrenamiento de la red neuronal\n",
    "# La normalización es crucial para convergencia estable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Conversión a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Baselines establecidos de regresión polinomial\n",
    "\n",
    "\n",
    "- mse_poly: Diccionario con errores cuadráticos medios de regresiones polinomiales previas\n",
    "- r2_poly: Diccionario con coeficientes de determinación r2 correspondientes\n",
    "\n",
    "en resumen estos son solo los resultados de la investigacion anteriror con regresion polinomila que usaremos para comparar mas adelante con el resultado de las redes neuronales\n",
    "\n",
    "\n",
    "- best_mse_poly = min(mse_poly.values())\n",
    "- best_r2_poly = max(r2_poly.values())\n",
    "\n",
    "aqui se encuuntra el menor valor del mse y el mayor de r2 esto pues para saber quue valores se deben comparar despues del analisis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly = {\n",
    "    \"Grado 1\": 17.7,\n",
    "    \"Grado 2\": 16.7,\n",
    "    \"Grado 3\": 16.7,\n",
    "    \"Grado 4\": 16.8,\n",
    "    \"Grado 5\": 17.4,\n",
    "}\n",
    "\n",
    "r2_poly = {\n",
    "    \"Grado 1\": 0.653,\n",
    "    \"Grado 2\": 0.673,\n",
    "    \"Grado 3\": 0.673,\n",
    "    \"Grado 4\": 0.671,\n",
    "    \"Grado 5\": 0.660,\n",
    "}\n",
    "\n",
    "# Mejor baseline anterior\n",
    "best_mse_poly = min(mse_poly.values())\n",
    "best_r2_poly = max(r2_poly.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Red neuroonal estandar\n",
    "\n",
    "- MPGNet: Red neuronal para regresión de MPG\n",
    "input_dim=7: 7 características de entrada cilindros, peso, etc.\n",
    "hidden_dims=[64, 32, 16]: la arquitectura que va reduciendo neuronas 7→64→32→16→1\n",
    "dropout_rate=0.2: 20% de neuronas se apagan para evitar sobreajuste\n",
    "\n",
    "\n",
    "### construccion de las capas - ciclo for \n",
    "\n",
    "\n",
    "en el loop fotr vamos a econtrar alguan fucniones que ayudan a confirgurar la red  por lo qeue me enfocare en rekacionar eso y en el loop \n",
    "\n",
    "- Bucle: Crea capas dinámicamente según hidden_dims\n",
    "- Linear: Conexiones  entre neuronas\n",
    "- ReLU: Función de activación convierte negativos en 0\n",
    "- Dropout: Regularización para prevenir memorización, es decir realiza una penalizacion para que el modelo se vea forzad oa encontrar mejores patrones \n",
    "- Salida: Una sola neurona valor de MPG predicho\n",
    "\n",
    "### inicializacion de los pesos\n",
    "\n",
    "- def _initialize_weights(self):\n",
    "\n",
    "\n",
    "esta fucnion inicializa los pesoso con valores optimos para evitar gradientes que desaparescan \n",
    "\n",
    "\n",
    "\n",
    "en conclusion este modelo define una red neuronaala de aruqitectuura 7→64→32→16→1 con regularización  y inicialización inteligente para predecir MPG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPGNet(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dims=[64, 32, 16], dropout_rate=0.2):\n",
    "        super(MPGNet, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        # Capas ocultas\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend(\n",
    "                [nn.Linear(prev_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "            )\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Capa de salida (regresión)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal simple \n",
    "\n",
    "\n",
    "- Shallow: Red  con solo 1 capa oculta\n",
    "- Arquitectura: 7→16→1 oseaa mas sencilla \n",
    "- Menos dropout: 0.1 (10%) en comparacion del 20% del modelo etandaar\n",
    "\n",
    "\n",
    "aqui este modelo nos servira para comparar si mas capas mejoran rendimiento y con menos sobreajuste puede  generalizar mejor con pocos datos\n",
    "\n",
    "\n",
    "Define una red neuronal minima 7→16→1 para probar si la simplicidad puede competir con arquitecturas más complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPGNetShallow(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dim=16, dropout_rate=0.1):\n",
    "        super(MPGNetShallow, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Red neuronal profunda\n",
    "\n",
    "\n",
    "- Deep: Red profunda 7→128→64→32→16→1\n",
    "- BatchNorm: estabiliza entrenamiento\n",
    "- Más dropout: 30% para red más compleja\n",
    "- 5 capas: Capacidad de aprender patrones complejos\n",
    "\n",
    "esta red sirve paea probar si más complejidad mejora predicciones y Ver límites de lo que puede lograr una red grande\n",
    "\n",
    "Definimos  una red neuronal muy profunda con una tecniica  (BatchNorm) para maximizar capacidad de aprendizaje y probar los límites del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPGNetDeep(nn.Module):\n",
    "    def __init__(self, input_dim=7, dropout_rate=0.3):\n",
    "        super(MPGNetDeep, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcioon de entrenamiento \n",
    "\n",
    "\n",
    "- MSELoss: penaliza errores grandes\n",
    "- Adam: optimizador adaptatif con weight_decay para regularizacion\n",
    "- loss_history: Guarda perdida de cada época para gráficos\n",
    "\n",
    "\n",
    "una vez se evalua no decolverra alguna  cosas en este caso son las siguientes \n",
    "\n",
    "- Historial de pérdida: Para gráficos de aprendizaje\n",
    "- MSE train/test: detecta el sobre ajute y guardaa el error de tst vs el de train\n",
    "- R2 train/test: Capacidad explicativa del modelo\n",
    "- Predicciones: para gráficos scatter y análisi\n",
    "\n",
    "en conclusion aqui tenmos la funcion de entrenamiento que funciona en cualquier modelo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=500, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    # Evaluación final\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = model(X_train).numpy()\n",
    "        test_pred = model(X_test).numpy()\n",
    "\n",
    "    # Métricas\n",
    "    mse_train = mean_squared_error(y_train.numpy(), train_pred)\n",
    "    mse_test = mean_squared_error(y_test.numpy(), test_pred)\n",
    "    r2_train = r2_score(y_train.numpy(), train_pred)\n",
    "    r2_test = r2_score(y_test.numpy(), test_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss_history\": loss_history,\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "        \"predictions\": test_pred,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de los 3 modelos \n",
    "\n",
    "\n",
    "### Modelo Shallow\n",
    "\n",
    "- menos épocas (300): red simple aprende más rápido\n",
    "- learning rate alto (0.01): pasos grandes porque es menos compleja\n",
    "\n",
    "### Modelo Standard\n",
    "\n",
    "- 500 épocas: tiempo estándar para lleagr a una convergencia\n",
    "- Learning rate medio (0.001): balance entre velocidad y estabilida\n",
    "\n",
    "### # Modelo Deep\n",
    "\n",
    "\n",
    "- Más épocas (700): Red compleja necesita más tiempo\n",
    "- Learning rate bajo (0.0005): Pasos pequeños para estabilidad\n",
    "\n",
    "\n",
    "en resumen aqui se entrenan lso  modelos y  laa pregunta aqui es ¿Por qué diferentes parámetros? es  por que las  redes más complejas requieren más tiempo y cuidado como ajustes específicos para cada arquitectura pues asi se optimiza el rendimiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Shallow\n",
    "model_shallow = MPGNetShallow()\n",
    "results_shallow = train_model(\n",
    "    model_shallow,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    epochs=300,\n",
    "    lr=0.01,\n",
    ")\n",
    "\n",
    "# Modelo Standard\n",
    "model_standard = MPGNet()\n",
    "results_standard = train_model(\n",
    "    model_standard,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    epochs=500,\n",
    "    lr=0.001,\n",
    ")\n",
    "\n",
    "# Modelo Deep\n",
    "model_deep = MPGNetDeep()\n",
    "results_deep = train_model(\n",
    "    model_deep,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    epochs=700,\n",
    "    lr=0.0005,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de resultados\n",
    "\n",
    "> en este caso no enfatizare en el codigo si no en el outpout recibido\n",
    "\n",
    "\n",
    "### Grafico 1 curvas de entrenameito \n",
    "\n",
    "- Shallow (azul): Converge rápidamente ~época 100, se estabiliza en MSE bajo\n",
    "- Standard (naranja): Convergencia similar al shallow, curva suave\n",
    "- Deep (verde): Converge más lentamente, necesita ~300 épocas\n",
    "\n",
    "\n",
    "Redes simples aprenden más rápido que las complejas, en teoria todas convergen bien aunque deep si necesita alguito de tiempo\n",
    "\n",
    "\n",
    "###  Scatter plot comparativos \n",
    "\n",
    "\n",
    "- eje X: MPG real del auto\n",
    "- eje Y: MPG predicho por cada modelo\n",
    "- linea roja diagonal: Predicción perfecta\n",
    "- proximidad a la línea: Mejor precisión\n",
    "\n",
    "entonces en base aa  los 3 graficos podemos definir \n",
    "\n",
    "- Shallow MSE: 8.54: puntos mas dispersos, mayor variabilidad\n",
    "- Standard MSE: 6.38: puntos mas  cerca de la diagonal\n",
    "- Deep MSE: 6.28: puntos mas concentrados cerca de la línea\n",
    "\n",
    "\n",
    "en conclusion Todos superan significativamente el baseline polinomial definido (16.7), la mejora Deep vs Standard es pequeña y Standard ofrece el mejor balance entre precisión y simplicidad segun lso resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Curvas de pérdida\n",
    "axes[0, 0].plot(results_shallow[\"loss_history\"], label=\"Shallow\", alpha=0.7)\n",
    "axes[0, 0].plot(results_standard[\"loss_history\"], label=\"Standard\", alpha=0.7)\n",
    "axes[0, 0].plot(results_deep[\"loss_history\"], label=\"Deep\", alpha=0.7)\n",
    "axes[0, 0].set_title(\"Curvas de Entrenamiento\")\n",
    "axes[0, 0].set_xlabel(\"Época\")\n",
    "axes[0, 0].set_ylabel(\"MSE Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Predicciones vs Reales - Shallow\n",
    "axes[0, 1].scatter(y_test, results_shallow[\"predictions\"], alpha=0.6, color=\"blue\")\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)\n",
    "axes[0, 1].set_title(f'Shallow Net (MSE: {results_shallow[\"mse_test\"]:.2f})')\n",
    "axes[0, 1].set_xlabel(\"MPG Real\")\n",
    "axes[0, 1].set_ylabel(\"MPG Predicho\")\n",
    "\n",
    "# Predicciones vs Reales - Standard\n",
    "axes[1, 0].scatter(y_test, results_standard[\"predictions\"], alpha=0.6, color=\"green\")\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)\n",
    "axes[1, 0].set_title(f'Standard Net (MSE: {results_standard[\"mse_test\"]:.2f})')\n",
    "axes[1, 0].set_xlabel(\"MPG Real\")\n",
    "axes[1, 0].set_ylabel(\"MPG Predicho\")\n",
    "\n",
    "# Predicciones vs Reales - Deep\n",
    "axes[1, 1].scatter(y_test, results_deep[\"predictions\"], alpha=0.6, color=\"orange\")\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)\n",
    "axes[1, 1].set_title(f'Deep Net (MSE: {results_deep[\"mse_test\"]:.2f})')\n",
    "axes[1, 1].set_xlabel(\"MPG Real\")\n",
    "axes[1, 1].set_ylabel(\"MPG Predicho\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla comparativa\n",
    "\n",
    "ah9ora bien  aqui solo contruimos la tabaal coparativa para anlizar los resultados de lo modelo  y de lo que teniamos anteriormente por lo que en la tabla se definio (baseline) \n",
    "\n",
    "- 4 columnas con diferentes colores y alineaciones\n",
    "\n",
    "- Primera parte: Agrega filas con resultados de regresión polinomial (baseline)\n",
    "- Segunda parte: Agrega filas con resultados de redes neuronales\n",
    "- Formato consistente: MSE con 1 decimal, R2 con 3 decimales\n",
    "\n",
    "\n",
    "### Analisis\n",
    "\n",
    "\n",
    "Las redes neuronales muestran una mejora respecto la regresión polinomial en la predicción de MPG, incluso con el modelo shallow que es el mas simple  superando al mejor polinomialMSE 16.7, R² 0.673 por un margen de 49% menos error y 20 puntos más de capacidad explicativa. , demostrando que las redes neuronales tienen una capacidad de modelado  mejor para capturar las relaciones complejas entre características automotrices del datacset  y eficiencia de combustible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = Table(\n",
    "    title=\"Comparación: Regresión Polinomial vs Redes Neuronales\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold\",\n",
    ")\n",
    "\n",
    "results_table.add_column(\"Modelo\", justify=\"left\", style=\"cyan\")\n",
    "results_table.add_column(\"Arquitectura/Grado\", justify=\"center\", style=\"yellow\")\n",
    "results_table.add_column(\"MSE Test\", justify=\"center\", style=\"green\")\n",
    "results_table.add_column(\"R² Test\", justify=\"center\", style=\"blue\")\n",
    "\n",
    "# Resultados anteriores (regresión polinomial)\n",
    "for grado, mse in mse_poly.items():\n",
    "    r2 = r2_poly[grado]\n",
    "    results_table.add_row(\"Regresión Polinomial\", grado, f\"{mse:.1f}\", f\"{r2:.3f}\")\n",
    "\n",
    "# Nuevos resultados (redes neuronales)\n",
    "results_table.add_row(\n",
    "    \"Red Neuronal\",\n",
    "    \"Shallow (7→16→1)\",\n",
    "    f'{results_shallow[\"mse_test\"]:.1f}',\n",
    "    f'{results_shallow[\"r2_test\"]:.3f}',\n",
    ")\n",
    "\n",
    "results_table.add_row(\n",
    "    \"Red Neuronal\",\n",
    "    \"Standard (7→64→32→16→1)\",\n",
    "    f'{results_standard[\"mse_test\"]:.1f}',\n",
    "    f'{results_standard[\"r2_test\"]:.3f}',\n",
    ")\n",
    "\n",
    "results_table.add_row(\n",
    "    \"Red Neuronal\",\n",
    "    \"Deep (7→128→64→32→16→1)\",\n",
    "    f'{results_deep[\"mse_test\"]:.1f}',\n",
    "    f'{results_deep[\"r2_test\"]:.3f}',\n",
    ")\n",
    "\n",
    "console = Console()\n",
    "console.print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafico de barra comparativo\n",
    "\n",
    "\n",
    "### GRÁFICO MSE (Izquierda)\n",
    "\n",
    "Barras rojas (polinomiales): Todas altas y similares 16.7-17.7\n",
    "Barras azules (redes): Todas bajas con progresión descendente 8.5→6.4→6.3\n",
    "\n",
    "\n",
    "### Grafico R2 (DERECHA)\n",
    "\n",
    "Barras rojas : Todas bajas y agrupadas 0.65-0.67\n",
    "Barras azules : Todas altas y progresiva 0.833→0.875→0.877\n",
    "\n",
    "\n",
    "en conclusion los gráficos confirman visualmente la que las redesdes neuronales son superirores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Preparar datos para gráficos\n",
    "modelos = [\n",
    "    \"Poli Grado 1\",\n",
    "    \"Poli Grado 2\",\n",
    "    \"Poli Grado 3\",\n",
    "    \"Poli Grado 4\",\n",
    "    \"Poli Grado 5\",\n",
    "    \"NN Shallow\",\n",
    "    \"NN Standard\",\n",
    "    \"NN Deep\",\n",
    "]\n",
    "\n",
    "mse_valores = [\n",
    "    17.7,\n",
    "    16.7,\n",
    "    16.7,\n",
    "    16.8,\n",
    "    17.4,\n",
    "    results_shallow[\"mse_test\"],\n",
    "    results_standard[\"mse_test\"],\n",
    "    results_deep[\"mse_test\"],\n",
    "]\n",
    "\n",
    "r2_valores = [\n",
    "    0.653,\n",
    "    0.673,\n",
    "    0.673,\n",
    "    0.671,\n",
    "    0.660,\n",
    "    results_shallow[\"r2_test\"],\n",
    "    results_standard[\"r2_test\"],\n",
    "    results_deep[\"r2_test\"],\n",
    "]\n",
    "\n",
    "# Colores diferenciados\n",
    "colors_mse = [\"lightcoral\"] * 5 + [\"lightblue\"] * 3\n",
    "colors_r2 = [\"lightcoral\"] * 5 + [\"lightblue\"] * 3\n",
    "\n",
    "# Gráfico MSE\n",
    "bars1 = ax1.bar(modelos, mse_valores, color=colors_mse, alpha=0.7)\n",
    "ax1.set_title(\"Error Cuadrático Medio (MSE)\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, valor in zip(bars1, mse_valores):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.1,\n",
    "        f\"{valor:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Gráfico R2\n",
    "bars2 = ax2.bar(modelos, r2_valores, color=colors_r2, alpha=0.7)\n",
    "ax2.set_title(\"Coeficiente de Determinación (R²)\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"R²\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, valor in zip(bars2, r2_valores):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{valor:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar mejor modelo\n",
    "all_models = [\n",
    "    (\"Polinomial Grado 2\", best_mse_poly, best_r2_poly),\n",
    "    (\"NN Shallow\", results_shallow[\"mse_test\"], results_shallow[\"r2_test\"]),\n",
    "    (\"NN Standard\", results_standard[\"mse_test\"], results_standard[\"r2_test\"]),\n",
    "    (\"NN Deep\", results_deep[\"mse_test\"], results_deep[\"r2_test\"]),\n",
    "]\n",
    "\n",
    "# Mejor por MSE menor es mejor\n",
    "best_model_mse = min(all_models, key=lambda x: x[1])\n",
    "# Mejor por R² mayor es mejor\n",
    "best_model_r2 = max(all_models, key=lambda x: x[2])\n",
    "\n",
    "print(f\"MEJOR MODELO MSE: {best_model_mse[0]} - MSE: {best_model_mse[1]:.2f}\")\n",
    "print(f\"MEJOR MODELO R2: {best_model_r2[0]} - R2: {best_model_r2[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en conclusion Se desarrolló una actividad de redes neuronales con PyTorch donde se implementaron dos modelos diferentes: una red neuronal de regresión con 3 capas ocultas que demostró mejor rendimiento que Ridge Regression en datos , y una red de clasificación binaria tipo XOR que logró alta precisión en la separación de clases no lineales. Los resultados mostraron que ambas redes funcionaron correctamente, aplicando técnicas de regularización como dropout y weight decay que permitieron obtener buenas métricas de evaluación (R2, F1-Score), confirmando que las arquitecturas neuronales pueden superar a modelos lineales en problemas complejos, tambien denotando que el emjor modelo fue el Deep."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
